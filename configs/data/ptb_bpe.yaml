_target_: src.data.DataModule
tokenizer:
  _target_: transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: bert-base-uncased
tokenize_transform:
  _target_: src.data.transforms.text.Tokenize
  tokenizer: ${data.tokenizer}
  tokenizer_args:
    return_special_tokens_mask: true
    truncation: true
    max_length: 512
mlm_collator:
  _target_: transformers.data.data_collator.DataCollatorForLanguageModeling
  tokenizer: ${data.tokenizer}
  mlm: true
  mlm_probability: 0.15
  pad_to_multiple_of: 1
train:
  dataset:
    _target_: datasets.load_dataset
    path: ptb_text_only
    split: train
  transform: ${data.tokenize_transform}
  loader:
    batch_size: ${batch_size}
    collate_fn: ${data.mlm_collator}
    shuffle: true
    drop_last: true
    num_workers: 8
    pin_memory: true
val:
  dataset:
    _target_: datasets.load_dataset
    path: ptb_text_only
    split: validation
  transform: ${data.tokenize_transform}
  loader:
    batch_size: ${eval_batch_size}
    collate_fn: ${data.mlm_collator}
    shuffle: false
    drop_last: false
    num_workers: 8
    pin_memory: true

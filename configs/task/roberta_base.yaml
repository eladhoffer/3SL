_target_: src.tasks.text.MaskedLanguageModelTask
model:
  _target_: src.models.huggingface.RobertaForMaskedLM
  config:
    _target_: transformers.RobertaConfig
    vocab_size: 50265
    tie_word_embeddings: true
optimizer:
  _target_: src.optim.OptimRegime
  regime:
    - epoch: 0
      optimizer: AdamW
      betas: (0.9, 0.98)
      eps: 1e-6
      weight_decay: 0.01
      step_lambda:
        _target_: src.optim.scheduler.linear_warmup_and_decay
        lr: 5e-4
        init_lr: 0.0
        warmup_steps: 8000 
        total_steps: ${trainer.max_steps}
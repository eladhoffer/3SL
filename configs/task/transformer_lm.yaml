_target_: src.tasks.text.LanguageModelTask
model:
  _target_: src.models.huggingface.RobertaForMaskedLM
  config:
    _target_: transformers.RobertaConfig
    vocab_size: 50265
    tie_word_embeddings: true
optimizer:
  _target_: src.optim.OptimConfig.instantiate
  optimizer:
    _target_: torch.optim.AdamW
    betas:
      - 0.9
      - 0.98
    eps: 1e-6
    weight_decay: 0.01
    lr: 1e-5
  lr_scheduler:
    _target_: src.optim.scheduler.LinearWarmUpDecay
    lr: 7e-4
    init_lr: 1e-5
    warmup_steps: 8000
    total_steps: ${trainer.max_steps}
  interval: step
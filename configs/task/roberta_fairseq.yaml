_target_: src.tasks.text.MaskedLanguageModelTask
model:
  _target_: src.models.fairseq.roberta
  vocab_dir: /home/ehoffer/PyTorch/fairseq_projects/roberta
optimizer:
  _target_: src.optim.OptimRegime
  regime:
    - epoch: 0
      optimizer: AdamW
      betas: (0.9, 0.98)
      eps: 1e-6
      weight_decay: 0.01
      step_lambda:
        _target_: src.optim.scheduler.linear_warmup_and_decay
        lr: 5e-4
        warmup_steps: 4000 
        total_steps: ${trainer.max_steps}
model_type: fairseq        
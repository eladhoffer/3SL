_target_: src.tasks.text.MaskedLanguageModelTask
model:
  _target_: transformers.RobertaForMaskedLM
  config:
    _target_: transformers.RobertaConfig
    num_hidden_layers: 6
optimizer:
  _target_: src.optim.OptimRegime
  regime:
    - epoch: 0
      optimizer: AdamW
      betas: (0.9, 0.98)
      eps: 1e-6
      weight_decay: 0.01
      step_lambda: "lambda t: {'lr': 1e-3 * min(t ** -0.5, t * 10000 ** -1.5)}"
label_smoothing: 0.1      

# @package _global_

# to execute this experiment run:
# python run.py experiment=roberta_base.yaml

defaults:
  - override /trainer: ddp.yaml # choose trainer from 'configs/trainer/'
  - override /task: roberta_base.yaml
  - override /data: wikibooks.yaml
  - override /callbacks: default.yaml
  - override /logger: many_loggers.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

seed: 12345
batch_size: 64
resume: null

trainer:
  max_steps: 125000
  max_epochs: ${trainer.max_steps}
  gpus: 8
  precision: 16
  accumulate_grad_batches: 4
  val_check_interval: 4000

data:
  train:
    loader:
      batch_size: 64
      num_workers: 0
      pin_memory: false
  val:
    loader:
      batch_size: 64
      num_workers: 0
      pin_memory: false

callbacks:
  model_checkpoint:
    monitor: "loss/val" 
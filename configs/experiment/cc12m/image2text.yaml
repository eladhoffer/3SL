# @package _global_

# to execute this experiment run:
# python run.py experiment=fixmatch_cifar10
defaults:
  - override /trainer: default.yaml # choose trainer from 'configs/trainer/'
  - override /data: cc12m_text.yaml
  - override /callbacks: default.yaml
  - override /logger: many_loggers.yaml
  - override /task: image2text.yaml
# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

seed: 12345
resume: null

trainer:
  min_epochs: 1
  max_steps: 50000
  max_epochs: ${trainer.max_steps}
  benchmark: false
  gpus: [0]
  precision: 16
  val_check_interval: 1000

data:
  train:
    loader:
      batch_size: 32
      num_workers: 4
      pin_memory: false
  val:
    loader:
      batch_size: 32
      num_workers: 4
      pin_memory: false

task:
  optimizer:
    _target_: src.optim.OptimConfig.instantiate
    optimizer:
      _target_: src.optim.lars.LARS
      lr: 1.0
      momentum: 0.9
      weight_decay: 0.0002
      nesterov: false
      trust_coefficient: 0.001
      eps: 1e-8
    optimizer_filters:
      - filter:
          _target_: src.optim.filter.OnlyBiasBN
          exclude: true
      - filter:
          _target_: src.optim.filter.OnlyBiasBN
        skip_scale: true          
    lr_scheduler:
      _target_: src.optim.scheduler.PolynomialWarmUpDecay
      lr: ${task.optimizer.optimizer.lr}
      warmup_steps: 1000
      total_steps: ${trainer.max_steps}
      init_lr: 0.
      final_lr: 0.0001
      decay_power: 2.0
    interval: step

callbacks:
  model_checkpoint:
    monitor: "loss/val"
    mode: "min"
    every_n_train_steps: 1000
    save_top_k: 2  


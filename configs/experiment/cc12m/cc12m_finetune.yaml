# @package _global_

# to execute this experiment run:
# python run.py experiment=fixmatch_cifar10
defaults:
  - override /trainer: ddp.yaml # choose trainer from 'configs/trainer/'
  - override /data: imagenet.yaml
  - override /callbacks: default.yaml
  - override /logger: many_loggers.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

seed: 12345
resume: null
epochs: 5
num_gpus: 2
batch_size: 256
accumulate_grad_batches: 1
global_batch_size: ${mul:${mul:${batch_size},${num_gpus}},${accumulate_grad_batches}}


trainer:
  max_epochs: ${epochs}
  benchmark: true
  precision: 16
  accumulate_grad_batches: ${accumulate_grad_batches}
  gpus: [2,3]

data:
  train:
    loader:
      batch_size: ${batch_size}
  val:
    loader:
      batch_size: ${batch_size}

task:
  _target_: src.tasks.supervised.FinetuneTask
  model:
    _target_: src.models.resnet.resnet
    dataset: imagenet
    num_classes: 768
    depth: 50
  optimizer:
    _target_: src.optim.OptimRegime
    regime:
      - epoch: 0
        optimizer: SGD
        lr: 0.1
        momentum: 0.9
        lr_scheduler:
          name: CosineAnnealingLR
          time_frame: step
          T_max: ${training_steps:${imagenet_size},${global_batch_size},${epochs}}
          eta_min: 0
  classifier:
    _target_: torch.nn.Linear
    in_features: 2048
    out_features: 1000
  checkpoint_path: /home/labuser/Pytorch/3SL/results/cc12m_10M_10epochs/checkpoints/last.ckpt
# @package _global_

# to execute this experiment run:
# python run.py experiment=fixmatch_cifar10
defaults:
  - override /trainer: default.yaml # choose trainer from 'configs/trainer/'
  - override /data: cifar10.yaml
  - override /callbacks: default.yaml
  - override /logger: many_loggers.yaml
  - override /task: default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

seed: 12345
resume: null

trainer:
  min_epochs: 1
  max_steps: 7812
  max_epochs: ${trainer.max_steps}
  benchmark: true

data:
  train:
    loader:
      batch_size: 64
  val:
    loader:
      batch_size: 512

optimizer_config:
  optimizer:
    _target_: torch.optim.SGD
    lr: 0.005
    momentum: 0.9
  lr_scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
    T_max: ${trainer.max_steps}
    eta_min: 0
  interval: step
  regularizer:
    _target_: src.optim.regularization.WeightDecay
    value: 5e-4
  filter:
    _target_: src.optim.filter.Filter
    module_name: classifier
    exclude: true 
classifier_optimizer_config:
  optimizer:
    _target_: torch.optim.SGD
    lr: 0.1
    momentum: 0.9
  lr_scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
    T_max: ${trainer.max_steps}
    eta_min: 0
  interval: step
  regularizer:
    _target_: src.optim.regularization.WeightDecay
    value: 5e-4
  filter:
    _target_: src.optim.filter.Filter
    module_name: classifier
task:
  _target_: src.tasks.supervised.FinetuneTask
  model:
    _target_: src.models.resnet.ResNet_cifar
    num_classes: 768
    width: [160, 320, 640]
    depth: 28
  optimizer:
    _target_: src.optim.OptimConfig.instantiate
    optimizer:
      _target_: torch.optim.SGD
      momentum: 0.9
    optimizer_filters:
      - filter:
          _target_: src.optim.filter.Filter
          module_name: classifier
          exclude: true
        lr: 0.005      
      - filter:
          _target_: src.optim.filter.Filter
          module_name: classifier
        lr: 0.1
    lr_scheduler:
      _target_: torch.optim.lr_scheduler.CosineAnnealingLR
      T_max: ${trainer.max_steps}
      eta_min: 0
    interval: step
    regularizer:
      _target_: src.optim.regularization.WeightDecay
      value: 5e-4
  classifier:
    _target_: torch.nn.Linear
    in_features: 640
    out_features: 10
  checkpoint_path: /home/labuser/Pytorch/3SL/results/cc12m/cc12m_small_mse/checkpoints/last.ckpt
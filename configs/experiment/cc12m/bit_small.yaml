# @package _global_

# to execute this experiment run:
# python run.py experiment=fixmatch_cifar10
defaults:
  - override /trainer: ddp.yaml # choose trainer from 'configs/trainer/'
  - override /data: cc5m_small.yaml
  - override /callbacks: default.yaml
  - override /logger: many_loggers.yaml
  - override /task: default.yaml
# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

seed: 12345
resume: null

trainer:
  min_epochs: 1
  max_steps: 19531
  max_epochs: ${trainer.max_steps}
  benchmark: true
  val_check_interval: 4000

data:
  train:
    loader:
      batch_size: 256
      num_workers: 4
  val:
    loader:
      batch_size: 256
      num_workers: 4
task:
  _target_: src.tasks.supervised.SupervisedEmbeddingTask
  model:
    _target_: timm.create_model
    model_name: resnetv2_50x1_bitm
    num_classes: 768
  optimizer:
    _target_: src.optim.OptimConfig.instantiate
    optimizer:
      _target_: torch.optim.AdamW
      betas:
        - 0.9
        - 0.98
      eps: 1e-6
      weight_decay: 0.001
      lr: 5e-4
    optimizer_filters:
      - filter:
          _target_: src.optim.filter.Filter
          parameter_name: bias
          exclude: true
      - filter:
          _target_: src.optim.filter.Filter
          parameter_name: bias
        weight_decay: 0.0     
    lr_scheduler:
      _target_: torch.optim.lr_scheduler.CosineAnnealingLR
      T_max: ${trainer.max_steps}
      eta_min: 0
    interval: step
  criterion:
    _target_: torch.nn.MSELoss
callbacks:
  model_checkpoint:
    monitor: "loss/val"
# @package _global_

# to execute this experiment run:
# python run.py experiment=resnet50_imagenet.yaml

defaults:
  - override /trainer: ddp.yaml # choose trainer from 'configs/trainer/'
  - override /task: default.yaml
  - override /data: imagenet.yaml
  - override /callbacks: default.yaml
  - override /logger: many_loggers.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

seed: 12345
batch_size: 64
num_gpus: 4
accumulate_grad_batches: 8
global_batch_size: ${mul:${mul:${batch_size},${num_gpus}},${accumulate_grad_batches}}
resume: null
epochs: 40
warmup_epochs: 5

trainer:
  max_epochs: ${epochs}
  gpus: ${num_gpus}
  precision: 16
  benchmark: true
  accumulate_grad_batches: ${accumulate_grad_batches}

data:
  train:
    loader:
      batch_size: ${batch_size}
  val:
    loader:
      batch_size: ${batch_size}

task:
  _target_: src.tasks.supervised.ClassificationWNoiseTask
  model:
    _target_: torchvision.models.resnet50
  label_smoothing: 0.1
  optimizer:
    _target_: src.optim.OptimConfig.instantiate
    optimizer:
      _target_: src.optim.lars.LARS
      lr: 8.5
      momentum: 0.9
      weight_decay: 0.0002
      nesterov: false
      trust_coefficient: 0.001
      eps: 1e-8
    optimizer_filters:
      - filter:
          _target_: src.optim.filter.OnlyBiasBN
          exclude: true
      - filter:
          _target_: src.optim.filter.OnlyBiasBN
        skip_scale: true
        weight_decay: 0.0         
    lr_scheduler:
      _target_: src.optim.scheduler.PolynomialWarmUpDecay
      lr: ${task.optimizer.optimizer.lr}
      warmup_steps: ${training_steps:${imagenet_size},${global_batch_size},${warmup_epochs}}
      total_steps: ${training_steps:${imagenet_size},${global_batch_size},${epochs}}
      init_lr: 0.
      final_lr: 0.0001
      decay_power: 2.0
    interval: step
# @package _global_

# to execute this experiment run:
# python run.py experiment=fixmatch_cifar10
defaults:
  - override /trainer: ddp.yaml # choose trainer from 'configs/trainer/'
  - override /data: imagenet.yaml
  - override /callbacks: default.yaml
  - override /logger: many_loggers.yaml
  - override /task: default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

seed: 12345
resume: null

trainer:
  min_epochs: 1
  max_steps: 10000
  max_epochs: ${trainer.max_steps}
  benchmark: true
  val_check_interval: ${trainer.max_steps}
  accumulate_grad_batches: 1
data:
  train:
    loader:
      batch_size: 32
      num_workers: 2
      pin_memory: false
  val:
    loader:
      batch_size: 32
      num_workers: 2
      pin_memory: false   

task:
  _target_: src.tasks.supervised.FinetuneTask
  model:
    _target_: src.models.resnet.resnet
    dataset: imagenet
    depth: 50
  optimizer:
    _target_: src.optim.OptimConfig.instantiate
    optimizer:
      _target_: torch.optim.SGD
      lr: 1e-3
      momentum: 0.9
    regularizer:
      _target_: src.optim.regularization.WeightDecay
      value: 1e-4      
    lr_scheduler:
      _target_: src.optim.scheduler.LinearWarmUpDecay
      lr: ${task.optimizer.optimizer.lr}
      init_lr: 1e-4
      final_lr: 1e-5
      warmup_steps: 500
      total_steps: ${trainer.max_steps}
    interval: step
  classifier: null
  remove_layer: null
  freeze_bn: false
  checkpoint_path: /home/ehoffer/PyTorch/3SL/results/FP8/fp8_srfb_checkpoint.pth
